{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$c = \\sqrt{a^2 + b^2}$$\n",
       "$$\\begin{array}{|c|c|c|}\n",
       "\\hline\\\\\n",
       "5&5&23523452\\\\\n",
       "\\hline\\\\\n",
       "5 & d & 3241\\\\\n",
       "\\hline\n",
       "\\end{array}$$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$$c = \\sqrt{a^2 + b^2}$$\n",
    "$$\\begin{array}{|c|c|c|}\n",
    "\\hline\\\\\n",
    "5&5&23523452\\\\\n",
    "\\hline\\\\\n",
    "5 & d & 3241\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx$$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import Math\n",
    "Math(r'$F(k) = \\int_{-\\infty}^{\\infty} f(x) e^{2\\pi i k} dx$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$${\\bf worksheets.codalab.org}$$\n",
       "\\[ x^n + y^n = z^n \\]\n",
       "\\begin{equation}\n",
       "\n",
       "\\int \\sum \\prod\n",
       "< > \\subset \\supset \\subseteq \\supseteq\n",
       "\\\\\\int_{a}^{b} x^2 dx\n",
       "\\\\ \\sum_{i=1}^{\\infty} \\frac{1}{n^s} \n",
       "\\\\ \\prod_{i=1}^n\n",
       "\\end{equation}\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$${\\bf worksheets.codalab.org}$$\n",
    "\\[ x^n + y^n = z^n \\]\n",
    "\\begin{equation}\n",
    "\n",
    "\\int \\sum \\prod\n",
    "< > \\subset \\supset \\subseteq \\supseteq\n",
    "\\\\\\int_{a}^{b} x^2 dx\n",
    "\\\\ \\sum_{i=1}^{\\infty} \\frac{1}{n^s} \n",
    "\\\\ \\prod_{i=1}^n\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "*Italics?*\n",
    "\n",
    "There are some things you might want to consider before you build your solution and raise venture capital:\n",
    "\n",
    "**Background:**\n",
    "\n",
    "\n",
    "\\- Daniel\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "<a href=\"http://oceanservice.noaa.gov/education/tutorial_currents/welcome.html\"> Ocean Service Currents Tutorial</a>\n",
    "<br>\n",
    "<a href=\"http://www.divediscover.whoi.edu/history-ocean/maury.html\"> Dive Discover</a>\n",
    "<br>\n",
    "<a href=\"http://www.seasky.org/ocean-exploration/ocean-explorers-matthew-maury.html\"> Sea Sky </a>\n",
    "<br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Matthew_Fontaine_Maury\">Wiki Matthew Maury</a>\n",
    "<br>\n",
    "<a href=\"http://oceanmotion.org/html/background/timeline1800.html\"> Ocean Motion Timeline</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "example_graph = \"1_example_graph.png\"\n",
    "graph = pydot.Dot(graph_type='digraph')\n",
    "node_a = pydot.Node(\"Node A\", style=\"filled\", fillcolor=\"red\")\n",
    "node_b = pydot.Node(\"Node B\", style=\"filled\", fillcolor=\"green\")\n",
    "node_c = pydot.Node(\"Node C\", style=\"filled\", fillcolor=\"#0000ff\")\n",
    "node_d = pydot.Node(\"Node D\", style=\"filled\", fillcolor=\"#976856\")\n",
    "graph.add_node(node_a)\n",
    "graph.add_node(node_b)\n",
    "graph.add_node(node_c)\n",
    "graph.add_node(node_d)\n",
    "graph.add_edge(pydot.Edge(node_a, node_b, label=\"5\"))\n",
    "graph.add_edge(pydot.Edge(node_a, node_c, label=\"2x\"))\n",
    "graph.add_edge(pydot.Edge(node_c, node_d, label=\"12\"))\n",
    "graph.add_edge(pydot.Edge(node_c, node_b, label=\"0\"))\n",
    "graph.add_edge(pydot.Edge(node_b, node_d, label=\"5x\", labelfontcolor=\"#009933\", fontsize=\"10.0\", color=\"blue\"))\n",
    "graph.write_png(str(example_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](./1_example_graph.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e04248db89a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mipyparallel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mipp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mipp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpid_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/ipyparallel/client/client.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url_file, profile, profile_dir, ipython_dir, context, debug, sshserver, sshkey, password, paramiko, timeout, cluster_id, **extra_args)\u001b[0m\n\u001b[1;32m    370\u001b[0m                         \u001b[0mno_file_msg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     ])\n\u001b[0;32m--> 372\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0murl_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_file_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: Connection file '~/.ipython/profile_default/security/ipcontroller-client.json' not found.\nYou have attempted to connect to an IPython Cluster but no Controller could be found.\nPlease double-check your configuration and ensure that a cluster is running."
     ]
    }
   ],
   "source": [
    "# throw this in terminal to kick cluster\n",
    "# ipcluster start -n 4\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "rc = ipp.Client()\n",
    "ar = rc[:].apply_async(os.getpid)\n",
    "pid_map = ar.get_dict()\n",
    "print pid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def PDF(url):\n",
    "    return HTML('<iframe src=%s width=700 height=350></iframe>' % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math and Stats \n",
    " * **Latent**: *existing but note yet developed or manifest, hidden, concealed.*\n",
    "     Latent variable model is a statistical model that relates a set of variables called manifest variables to a set of latent variables. [Latent Variable Model](https://en.wikipedia.org/wiki/Latent_variable_model)\n",
    "\n",
    "| Continuous | Categorical\n",
    "------------ | ------------- | ---------------\n",
    "Latent Variables | Factor Analysis | Item response theory \n",
    "Continuous|  Latent Profile Analysis| Latent class analysis\n",
    "\n",
    "\n",
    "\n",
    " * **Intractable**: *Problems that can be solved in theory (e.g., given large but finite time), but which in practice take too long for their solutions to be useful, are known as intractable problems.[11] In complexity theory, problems that lack polynomial-time solutions are considered to be intractable for more than the smallest inputs.  *\n",
    " * **Inference**: *a conclusion reached on the basis of evidence and reasoning.*\n",
    " * **Bayesian Probability**: \n",
    "     * H stands for hypothesis and E stands for evidence.\n",
    "     * P(H) the prior probability, the probability of H before E is observed.\n",
    "     * P(H|E) the posterior probability (Goal: probability of hypothesis given the evidence)\n",
    "     * P(E|H) is the probability of observing E given H. As a function of  H with E fixed, this is the likelihood.\n",
    "     * P(E) is called marginal likelihood. This factor is the same for all possible hypotheses being considered. \n",
    " \\begin{equation}\n",
    "P(H | E) = \\frac{P(E | H)*P(H)}{P(E)} \\\\\n",
    "\\end{equation}\n",
    "\n",
    " * Markov Random vs Conditional Random Field:\n",
    " * Transfer Learning in Reinforcement Learning:\n",
    " * Guided Policy Search:\n",
    " * MCMC or Markov Chain Monte Carlo:\n",
    " * [EM (Expected Maximization) algorithm](http://cs229.stanford.edu/notes/cs229-notes8.pdf)\n",
    " * **VGG**: Visual Geometry Group from Oxford submitted Very Deep ConvNets for Large-Scale Image Recognition\n",
    "    [Youtube Talk](https://www.youtube.com/watch?v=j1jIoHN3m0s) [Paper](http://arxiv.org/pdf/1409.1556.pdf)\n",
    " * **KL Divergence**:\n",
    "\\begin{equation}\n",
    "\\sum_{i} P(i)\\log\\frac{P(i)}{Q(i)} \\\\\n",
    "\\int_{-\\infty}^{\\infty} p(x) \\log\\frac{p(x)}{q(x)} dx\n",
    "\\end{equation}\n",
    "\n",
    "        In words, it is the expectation of the logarithmic difference between the probabilities P and Q, where the expectation is taken using the probabilities P. The Kullback–Leibler divergence is defined only if Q(i)=0 implies P(i)=0, for all i (absolute continuity). Whenever P(i) is zero the contribution of the i-th term is interpreted as zero because. For distributions P and Q of a continuous random variable, the Kullback–Leibler divergence is defined to be the integral: where p and q denote the densities of P and Q.\n",
    "        \n",
    "        The Kullback-Leibler divergence is the penalty you'll have to pay if you try to compress data from one distribution using a scheme optimised for another. More precisely: if your data really comes from probability distribution P, but you use a compression scheme optimised for Q, the divergence D(P||Q) is the number of extra bits you'll require to store a record of each sample from P. D(Q||P) ≥ 0 because by definition it's the number of bits required compared to the best possible. D(P||P) = 0 because you don't need any extra bits if you are using the optimal scheme. But there's no reason to expect D(P||Q) to equal D(Q||P) because they are answering different questions about encoding data using different compression schemes.\n",
    "    \n",
    "    \\lim_{x\\to 0}x\\log(x)=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " * Tensor Train Decomposition:\n",
    " * [Cifar](https://www.cs.toronto.edu/~kriz/cifar.html): The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    " * **i.i.d**: Independent and indentically distributed random variables from distribution. If each random variable has the same probability distribution as the others and all are mutually independent\n",
    " * Twitter Autograd:\n",
    " * Cort.io:\n",
    " * Diversity Essay for berkeley and stanford:\n",
    " * Regret bound online learning:\n",
    " * IDEAS: Embedding of science papers with embedded ontology of citations:\n",
    " * Sheffield:\n",
    " * Honlak Lee:\n",
    " * Make Email for Interested in Ph.D Program: \\\\\n",
    "   Interested in your research these papers in particular. wondering about where research is headed\\\\\n",
    "   how is the college/department environment collaborative, friendly? \\\\\n",
    "   thoughts on what to get a PhD in Machine Learning, Deep Learning, NLP, !ask Google Guy!\n",
    "\n",
    " * Look up slides from hebies/ harvard guy\n",
    " * Hierchachical Softmax\n",
    " * Pascal Paper, what is spherical family?\n",
    " * Equilibration non convex paper:\n",
    "\n",
    " * On the job learning cost optimization:\n",
    "\n",
    " * PGM:\n",
    "\n",
    " * Autonomy:\n",
    " * Online Learning Simulation for Stanford NLP Crowdsource Paper, make fake results on a gaussian:\n",
    " * Which units in deep learning are reduntant:\n",
    " * Train just gates?:\n",
    " * Roperator Hv^2:\n",
    " * RMS prop vs adagrad:\n",
    " * tell Murat for Cloud Computing Academic Database/Code Automator:\n",
    " * Hilbert Space: A Hilbert space H is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product.[2] To say that H is a complex inner product space means that H is a complex vector space on which there is an inner product \\langle x,y\\rangle associating a complex number to each pair of elements x,y of H that satisfies the following properties:\n",
    "\n",
    "\n",
    " * [Nextml.org](http://nextml.org): Project for online learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Empirical cost vs expected loss\n",
    "* Make things differentiable so you can back prop through them\n",
    "* Encoding molecule as string, company is called atomwise from toronto\n",
    "* admm -> convex\n",
    "* Use dictionary of parts of speech as something to feed to convultion mode\n",
    "* reccommend papers.org?\n",
    "* Learn animal hierachy images?\n",
    "* Batch normalization, spatial transformer\n",
    "* Adam ?\n",
    "* Covariate shift\n",
    "* Ladder Networks, Curious ai (Unsupervised)\n",
    "* Denoising Auto encoder\n",
    "* Recurrent Ladder\n",
    "* Self Taught Learning honglak\n",
    "* Finad analysis of face by neuron network cukh.edu 80% for pruning\n",
    "* Name that dataset Game: everyone knows the datasets and thats a problem with overfitting\n",
    "* Compositionallity in VQA Darrel/ jacob andreas paper\n",
    "* Segmentaiton Natural language object retrieval\n",
    "* Get dataset on how people descrive a scene or ask questions about liek the box parser move game in 60's\n",
    "* Fundamental Provlems\n",
    "    * convulution nets that take advantage of symmetry\n",
    "    * evaluate unsupervised learning\n",
    "        * show generated results to people\n",
    "        * show how to transfer to other tasks\n",
    "        * how much memory(overfitting) is needed in the model\n",
    "    * Unsupervised learning is compression for the most part\n",
    "* Think of child language acquisition as an infinite cache that the child builds.\n",
    "* Look neural teuring machine\n",
    "* Curriculum learning? location based attention\n",
    "* Find LM slides from Symposium Kim Rush Charcnn baselines\n",
    "* Can you train NER with CNN on words?\n",
    "* LSTM for language modelling for NER LING et Al\n",
    "* Pieter Abhel\n",
    "* RL: Credit assignment problem with the maze goal. How to nicely give reward\n",
    "    * is RL nonconvex?\n",
    "* Competitive RL, if you have many things doing SGD but they are using each others information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### RL slides from Symposium Guy\n",
    "##### Leon Deep Art bethgelab.org\n",
    "##### Content + style = image = feel\n",
    "##### Author style + content = Text. Covariance matrix is style texture\n",
    "##### Compression of video by giving model to user and sending only the embedding?\n",
    "##### eyescream soumith.ch\n",
    "##### harmonic mean between handcoded algorithm and a rl for darpa walking\n",
    "##### Start reading Naacl submission ahead of time\n",
    "##### Deep Learning has worked in NER, MT, LM\n",
    "##### Get her stevenson to teach child acquisition class\n",
    "##### Goodfellow hyperparameter search with blurring\n",
    "##### Percy coda in stanford thing\n",
    "##### what is structured representation?\n",
    "##### Paul Smolensky representing thing in high dimension\n",
    "##### Reward for dialogue system: how long you can get a person to ineract with you\n",
    "##### Movie speaker prediction: and speaker change\n",
    "    * use the supreme court database, spoken language\n",
    "##### Next utterance prediction Ubuntu data nlp Pine dialogue 2015\n",
    "##### slot filling tasks\n",
    "##### Sent to sent prediction, two sent followed by two sent\n",
    "##### Symmetrical convolutions\n",
    "##### get snowflake dataset\n",
    "##### speech translational\n",
    "##### Recurrent CNN/LSTM/GRU\n",
    "##### Gradient change between Images in Video\n",
    "##### bidirectional rnn works better for text usually\n",
    "##### new dataset = state of the art paper\n",
    "##### bleu score refresher\n",
    "##### siamese network vs generative discrimanation\n",
    "##### Lexical embedding paper: out of embedding best sensei\n",
    "##### RNN on the time stuff angel did\n",
    "##### spectral lda: tensor spark\n",
    "##### admm has an unnecessary constraint elastic gradient descent 8 gpu 150 hours imagenet\n",
    "##### Target prop: lagragian formulation of backprop\n",
    "##### ask boris for paper on figuring out shadows from rays\n",
    "##### grid compression visual of an image\n",
    "##### spectral decompose/methods\n",
    "##### Set Cover problem: \n",
    "    Given a set of elements {1,2,3...n} called the universe and a collection of S of n sets whose union equals the universe, identify the smalled sub-collection of n whose union equals the universe. NP Complete Problem. The optimization version is NP-hard.\n",
    "    *set problem Pedro * rewatch the conversation \n",
    "##### To move in lots of science directions we need DL to have interpretability\n",
    "##### Degenearate vs non degenerate\n",
    "##### we should learn things that we people can learn\n",
    "##### stanford dude with frizzy hair who is it\n",
    "##### continuous vs discrete optimization\n",
    "##### minimax vs maxmin?\n",
    "##### [Sum-Product Networks: A New Deep Architecture](http://homes.cs.washington.edu/~pedrod/papers/uai11a.pdf)\n",
    "##### [Algorithmic Aspects of Machine Learning](http://people.csail.mit.edu/moitra/docs/bookex.pdf)\n",
    "[Class](http://people.csail.mit.edu/moitra/409.html)\n",
    "##### method moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Theory\n",
    "#### No Free Lunch \n",
    "    * Generilization accuraccy of Learner L = Accuracy of L on non-training examples\n",
    "\\begin{equation}\n",
    "Acc_G(L)\n",
    "\\end{equation}\n",
    "    * $F$ = Set of all possible concepts(boolean function to boolean output), $y$ = $f$($x$)\n",
    "         * x is example seen\n",
    "         * y is value of example\n",
    "    * Theorem For any learner L,\n",
    "\\begin{equation}\n",
    " \\frac{1}{|F|}\\sum_{F} Acc_G(L) = 1/2\n",
    "\\end{equation}\n",
    "given any distribution $D$ over $x$ and training set size $n$. **Corollary** for any two learners $L_1$, $L_2$:\n",
    "\n",
    "*Proof of Sketch* Given any training set $S$\n",
    "\\begin{equation}\n",
    "\\text{For every concept } f \\text{ where the } \\\\ Acc_G(L) = 1/2 + \\delta \\\\\n",
    "\\text{there is a concept }f' \\text{ where } \\\\ Acc_G(L) = 1/2 - \\delta\n",
    "\\end{equation}\n",
    "*Directly follows*\n",
    "\\begin{equation}\n",
    "\\text{if } \\exists \\text{ learning problem s.t.} \\\\ Acc_G(L_1) > Acc_G(L_2) \\\\\n",
    "\\text{then } \\exists \\text{ learning problem s.t.}\\\\ Acc_G(L_2) > Acc_G(L_1)\n",
    "\\end{equation}\n",
    "    * These two learners have seen exactly the same training data but have the opposite testing data.\n",
    "    \n",
    "    Reason why this doesn't destroy machine learning in the real world not all concepts are equally likely, because the real world is not randomly selected but most of the time its not because there is very general regularities.\n",
    "    In real world this is not uniform: \n",
    "$$\\frac{1}{|F|}$$\n",
    "\n",
    "----\n",
    "#### Bias and Variance\n",
    "Training set: \n",
    "$${(x_1,t_1)...(x_n,t_n)}$$\n",
    "Learner induced model $$y = f(x)$$\n",
    "* Squared Loss (mean) $$L(t,y) = (t-y)^2$$\n",
    "* Absolute Loss (median) $$L(t,y) = |t-y|$$\n",
    "* Zero-one Loss (mode) $$L(t,y) = 0 \\text{ if } y=t, 1 \\text{ otherwise}$$\n",
    "* Loss = Bias + Variance + Noise(Unavoidable Error)\n",
    "    * Bias is the difference between the Truth and the Average Prediction\n",
    "        * Generalized definition: loss incurred by main prediction\n",
    "    * Variance is the difference between average prediction and a particular prediction\n",
    "        * Generalized Definition: average loss incurred by prediction relative to main prediction\n",
    "    * In zero-one loss you can show that increasing the variance can reduce loss. In classification variance can be good in regresison variance is never good.\n",
    "![Decomposition](https://d396qusza40orc.cloudfront.net/machlearning/slides/1e9743c2-67d3-430b-93e4-97ca79002e00.wmv/slide_0025_full.jpg)\n",
    "\n",
    "1......................            | 2.............             |   3 \n",
    "------------ | ------------- | ----\n",
    "$$(a-b)^2$$ | L(a,b) | Loss\n",
    "$$E[(t-y)^2]$$ | $$L[(t,y)]$$ | Exp. Loss\n",
    "$$(t-\\bar{y})^2$$ | $$L(t,\\bar{y})$$ | Bias\n",
    "$$E[(\\bar{y}-y)^2]$$ | $$L(\\bar{y},y)$$ | Variance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### PAC (Probably Approximately Correct) Learning\n",
    "* Key contriubtion is to tell you how many samples are need to achieve a worst case accuracy\n",
    "* The Hypothesis space grows with $$2^{2^{d}}$$\n",
    "* Version space is all hypothesis that are right for all training examples\n",
    "* Hypothesis is required to be right for all training examples\n",
    "* Learner picks one hypothesis from the hypothesis space\n",
    "* The Cardinality of H is used to Union Bound the proof\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Hypothesis space H is finite and D is a sequence of m ≥ 1 independent random examples} \\\\ \\text{ of some target c, then for any 0 ≤} \\epsilon \\text{ ≤ 1, the probability that } VS_{H,D} \\text{(version space) contains a hypothesis with error greater than } \\epsilon \\text{ is less than: } \\\\\n",
    "|H|e^{-\\epsilon *m}\n",
    "\\end{equation}\n",
    "* We want to ensure with probability 95% (delta = .05), VS to contain hypotheses with error ≤ 10% (epsilon)\n",
    "    * approximately is epsilon\n",
    "    * probably is delta\n",
    "    * Pac Learnable **Must be in polynomial time**\n",
    "\\begin{equation}\n",
    "|H|e^{-\\epsilon * m} ≤ \\delta \\\\ \n",
    "m ≥ \\frac{1}{\\epsilon}(\\ln{|H|} + \\ln{\\frac{1}{\\delta}})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Agnostic Learning\n",
    "* Suppose the target consume is not in Hypothesis Space $$ c \\notin H$$ \n",
    "    * Hoeffding bound ???\n",
    "\n",
    "#### VC (Vapnik–Chervonenkis) Dimension\n",
    "* Infinite Hypothesis Spaces(Perceptrons, Real Numbers etc)\n",
    "* Intuition is if the hypothesis is able to seperate the space perfectly. Don't have infinite Data\n",
    "* A **dichotomy** of a set is a partion of S into two disjoint subsets\n",
    "* A set of instance of S is **shattered** by a **Hypothesis Space** iff for every dichotomy of S there exists some hypothesis in H consistent with this dichotomy.\n",
    "* VC Dimension of a Hypothesis space $$VC(H)$$ defined over instance space $X$ is the size of the **largest finite subset of X shattered by H**. \n",
    "* If arbitrarily large finite sets of X can be shattered by H then $$ VC(H) \\equiv \\infty $$ \n",
    "* XOR(4 points) breaks the Hyperplane seperation\n",
    "* VC Dimension of lines in the plane is 3(at most 3 points can be shattered)\n",
    "\\begin{equation}\n",
    "m ≥ \\frac{1}{\\epsilon}*(4\\log_2{\\frac{2}{\\delta}} + 8VC(H)*\\log_2{\\frac{13}{\\epsilon}})\n",
    "\\end{equation}\n",
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First Header | Second Header\n",
    "------------ | -------------\n",
    "Content from cell 1 | Content from cell 2\n",
    "\n",
    "\n",
    "# Non Convex Optimization\n",
    "\n",
    "Convolutional Dictionary Learning through Tensor Factorization. Furong Huang, [Anima Anandkumar](http://newport.eecs.uci.edu/anandkumar/)\n",
    "\n",
    "Guaranteed Matrix Completion via Non-convex Factorization. Ruoyu Sun, Zhi-Quan Luo.\n",
    "\n",
    "Spectral Clustering as Basis Recovery. James Voss, Mikhail Belkin, Luis Rademacher.\n",
    "\n",
    "Scalable Training of Interpretable Spatial Latent Factor Models. Stephan Zheng, Yisong Yue.\n",
    "\n",
    "Robust Tensor Decomposition under Block Sparse Perturbations. Animashree Anandkumar, Prateek Jain, Yang Shi, Niranjan Uma Naresh.\n",
    "\n",
    "Robust Regression via Hard Thresholding. Kush Bhatia, Prateek Jain, Purushottam Kar.\n",
    "\n",
    "Stochastically Transitive Models for Pairwise Comparisons: Statistical and Computational Issues. Nihar B. Shah, Sivaraman Balakrishnan, Adityanand Guntuboyina, Martin J. Wainwright.\n",
    "\n",
    "Statistical and Computational Guarantees for the Baum-Welch Algorithm. Fanny Yang, Sivaraman Balakrishnan, Martin J. Wainwright.\n",
    "\n",
    "On consistency of generalized Orthogonal Matching Pursuit. Francois-Xavier Dupe.\n",
    "\n",
    "Learning Latent Variable Models by Improving Spectral Solutions with Exterior Point Methods. Amirreza Shaban, Mehrdad Farajtabar, Bo Xie, Le Song, Byron Boots.\n",
    "\n",
    "When Are Nonconvex Problems Not Scary? Ju Sun, Qing Qu, John Wright.\n",
    "\n",
    "On Lloyd’s algorithm: new theoretical insights for clustering in practice. Cheng Tang, Claire Monteleoni.\n",
    "\n",
    "#### [Representation Results & Algorithms for Deep Feedforward Networks](http://cseweb.ucsd.edu/~mtelgars/manuscripts/nc15.pdf) Jacob Abernethy, Alex Kulesza, Matus Telgarsky.\n",
    "\n",
    "    *Summary*\n",
    "        * In other words, we can construct data sets for which a network of depth 2k gives zero error, but any\n",
    "    flat network has error at least 1/6 unless it contains an exponential number of nodes...The first part of the result guarantees learning a compact network over a restrictive class of functions, while the second shows that the algorithm learns more generally given a sufficiently large network... The upper and lower bound follow from a simple intuition: adding piecewise affine functions together grows the number of “bumps” only linearly, whereas composing them increases the number of bumps multiplicatively.\n",
    "\n",
    "    *Questions*\n",
    "        * T-sawtooth: if it is piecewise affine with t pieces, meaning R is partitioned\n",
    "    into t consecutive intervals, and σ is affine within each interval.\n",
    "\n",
    "    *Ideas*\n",
    "        * ?\n",
    "\n",
    "\n",
    "Robust Lp-norm Singular Value Decomposition. Kha Gia Quach, Khoa Luu, Chi Nhan Duong, Tien D. Bui.\n",
    "\n",
    "The Continuous Allreduce Algorithm for Asynchronous Stochastic Gradient Descent. Vijay Saraswat, Joshua Milthorpe.\n",
    "\n",
    "Provable bounds for nonconvex problems via new assumptions - [Sanjeev Arora](https://www.cs.princeton.edu/~arora/courses.html)\n",
    "\n",
    "\n",
    "Tackling Nonconvex Optimization by Complexity Progression - [Hossein Mobahi](http://people.csail.mit.edu/hmobahi/index2.html)\n",
    "\n",
    "Understanding Convergence with Fractional Hypertree Width and Martingale Methods - [Chris Re](http://cs.stanford.edu/people/chrismre/) [deepdive](http://deepdive.stanford.edu/)\n",
    "\n",
    "Panel discussion on open problems\n",
    "\n",
    "Recovering structured probability matrices -[Gregory Valiant](http://theory.stanford.edu/~valiant/pap_link.html)\n",
    "[Stoc workshop on efficient distribution estimation](http://www.iliasdiakonikolas.org/stoc-distribution-estimation.html) [stoc slides](http://www.columbia.edu/~cs2035/stoc/stoc2014/tutorials.shtml#ede)\n",
    "\n",
    "# Deep Learning\n",
    "\n",
    "#### [Visualizing Data using t-SNE, Laurens van der Maaten, Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/absps/tsne.pdf)\n",
    "\n",
    "\n",
    "# Reinforcement Learning\n",
    "# Dialogue Systems\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "def elina_age():\n",
    "  return date.today().year - 2009\n",
    "\n",
    "a = elina_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "variables": {
     "a": "7"
    }
   },
   "source": [
    "Elina is {{a}} year's old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "b = 10000000\n",
    "result = 0\n",
    "for a in tqdm(xrange(b)):\n",
    "    result += a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_graph = \"COLT_Game_Theory_1.png\"\n",
    "graph = pydot.Dot(graph_type='digraph')\n",
    "node_a = pydot.Node(\"Node A\", style=\"filled\", fillcolor=\"red\")\n",
    "node_b = pydot.Node(\"Node B\", style=\"filled\", fillcolor=\"green\")\n",
    "node_c = pydot.Node(\"Node C\", style=\"filled\", fillcolor=\"#0000ff\")\n",
    "node_d = pydot.Node(\"Node D\", style=\"filled\", fillcolor=\"#976856\")\n",
    "graph.add_node(node_a)\n",
    "graph.add_node(node_b)\n",
    "graph.add_node(node_c)\n",
    "graph.add_node(node_d)\n",
    "graph.add_edge(pydot.Edge(node_a, node_b, label=\"5\"))\n",
    "graph.add_edge(pydot.Edge(node_a, node_c, label=\"2x\"))\n",
    "graph.add_edge(pydot.Edge(node_c, node_d, label=\"12\"))\n",
    "graph.add_edge(pydot.Edge(node_c, node_b, label=\"0\"))\n",
    "graph.add_edge(pydot.Edge(node_b, node_d, label=\"5x\", labelfontcolor=\"#009933\", fontsize=\"10.0\", color=\"blue\"))\n",
    "graph.write_png(str(example_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Applications of Learning Theory in Algorithmic Game Theory](http://videolectures.net/colt2015_roughgarden_game_theory/)\n",
    "* Price of Anarchy is Deity Solution / Nash Equilibrium\n",
    "    * if multiple equilibriums then worst one is used\n",
    "* No Regret Guarantee: weaker equilibrium = \n",
    "![Price of Anarchy](COLT_Game_Theory_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Logic and Graphs\n",
    "##### Motivation\n",
    "##### [Markov Logic Networks](http://www.cs.kun.nl/~peterl/teaching/CI/markovlogic2006.pdf)\n",
    "Undirected Graphical Model.\n",
    "Combine logic knowledge base and markov network\n",
    "#####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Understanding\n",
    "#### Percy ICML Tutorial [Slides](http://icml.cc/2015/tutorials/icml2015-nlu-tutorial.pdf) [Video](http://videolectures.net/icml2015_liang_language_understanding/)\n",
    "[Lexical Relationships](http://www.slideshare.net/honeyravian1/lexical-relations)\n",
    "Pragmatics: what does it do?\n",
    "Semantics: what does it mean?\n",
    "Syntax: what is grammatical?\n",
    "\n",
    "Levels | Linguistics | Programming Languages\n",
    "----------| ------------ | -------------\n",
    "Pragmatics | what does it do? |  implemented the right algorithm\n",
    "Semantics| what does it mean? |  no implementation bugs\n",
    "Syntax |what is grammatical? | no compiler errors\n",
    "\n",
    "Anaphora\n",
    "* The dog chased the cat, which ran up a tree. It waited at the top.\n",
    "* The dog chased the cat, which ran up a tree. It waited at the bottom.\n",
    "* [The Winograd Schema Challenge (Levesque, 2011)](http://www.cs.toronto.edu/~hector/Papers/winograd.pdf)\n",
    "\n",
    "##### Pragmatics\n",
    "* Conversational implicature: new material **suggested** (not logically implied)\n",
    "    * What on earth has heppened to the roast beef?\n",
    "    * The Dog is looking very happy\n",
    "    * **Implicature:** the dog ate the roast beef\n",
    "* Presupposition: background **assumption** independent of truth of sentence\n",
    "    * When did you stop cheating on your wife?\n",
    "    * I have stopped eating meat\n",
    "\n",
    "* Reasons NLU is hard\n",
    "    * Vagueness: does not specify full information\n",
    "        * I had a late lunch.\n",
    "    * Ambiguity: more than one possible (precise) interpretations\n",
    "        * One morning I shot an elephant in my pajamas.\n",
    "        * How he got in my pajamas, I don’t know. — Groucho Marx\n",
    "    * Uncertainty: due to an imperfect statistical model\n",
    "        * The witness was being contumacious.\n",
    "---\n",
    "##### Distributional semantics\n",
    "* The new design has ____ lines.\n",
    "* Let’s try to keep the kitchen ____.\n",
    "* I forgot to ____ out the cabinet.\n",
    "* What does ____ mean?\n",
    "**Idea** Form a word-context Matrix and then do dimensionality reduction(generalize) on it\n",
    "* Famous example is LSA(document wise) which uses SVD\n",
    "* Skip-gram Model (Word2Vec) Dimensionality reduction (Logistics regression with SGD)\n",
    "    * Model: predict good (w, c) using logistic regression\n",
    "    $$ p_{θ}(g = 1 | w, c) = (1 + exp(θ_{w} · β_{c}))−1 $$\n",
    "    * Positives: (w, c) from data\n",
    "    * Negatives: (w, c') for irrelevant c' (k times more)\n",
    "    $$ +(cats, AI)  −(cats, linguistics)  −(cats, statistics)$$\n",
    "    \n",
    "Based purely on statistical effects\n",
    "\n",
    "---\n",
    "##### Frame semantics\n",
    "\n",
    "Frame semantics: meaning given by a frame, a stereotypical situation\n",
    "Sold invokes a Seller:?, Buyer:?, Goods:?, Price:?\n",
    "Based on previous knowledge\n",
    "* FrameNet\n",
    "    * Has frames with lexical(words +pos) units that trigger the frame.\n",
    "    * 4k Predicates\n",
    "* PropBank\n",
    "    * Centered around verbs and syntax\n",
    "    * Word senses tied to WordNet\n",
    "\n",
    "* **Task: Semantic Role Labeling**\n",
    "\n",
    "| Input:  Cynthia sold the bike to Bob for \\$200 |\n",
    "| ------------------|\n",
    "| Output: SELLER PREDICATE GOODS BUYER PRICE |\n",
    "\n",
    "1. Construct dependency parse, choose predicate p (bought)\n",
    "2. Extract paths from p to dependents a\n",
    "3. Map each dependent a to vector (word vectors) $$v_{a} $$ \n",
    "4. Compute low. dim. representation $$ φ = M[v_{a1}, . . . , v_{an}]$$\n",
    "5. Predict score  for label y (e.g., buy.01) $$φ · θ_{y}$$\n",
    "\n",
    "- Learn parameters {vw}, M, {θy} from full supervision\n",
    "- This is a structured prediction task\n",
    "\n",
    "###### [A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning: No feature engineering or parse trees Collobert/Weston, 2008](http://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf)\n",
    "\n",
    "###### [Embeddings for frame identification Hermann/Das/Weston/Ganchev,2014](http://www.dipanjandas.com/files/acl2014frames.pdf)\n",
    "\n",
    "- Motivation for AMR: unify all semantic annotation\n",
    "    - There is Semantic Role Labeling\n",
    "    - Named Entity Recognition\n",
    "    - Coreference Resolution\n",
    "\n",
    "---\n",
    "\n",
    "##### Model-theoretic semantics\n",
    "- Model theory: interpretation depends on the world state\n",
    "- Compositionality: meaning of whole is meaning of parts\n",
    "- \n",
    "\n",
    "<img  src=\"components of semantic parser.png\" style=\"width:600px;height:400px;\"/>\n",
    "\n",
    "\n",
    "---\n",
    "##### Reflections\n",
    "\n",
    "1. Distributional semantics:\n",
    "• Pro: Most broadly applicable, ML-friendly\n",
    "• Con: Monolithic representations\n",
    "2. Frame semantics:\n",
    "• Pro: More structured representations\n",
    "• Con: Not full representation of world\n",
    "3. Model-theoretic semantics:\n",
    "• Pro: Full world representation, rich semantics, end-to-end\n",
    "• Con: Narrower in scope\n",
    "\n",
    "**Instead of trying to produce a programme to simulate the adult mind,\n",
    "why not rather try to produce one which simulates the child’s?** Quote\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured Belief Propagation\n",
    "##### Structured Belief Propagation for NLP, Matthew Gormley, Jason Eisner [video](http://techtalks.tv/talks/structured-belief-propagation-for-nlp-part-1/62082/)   [slides](http://www.cs.jhu.edu/~mrg/bp-tutorial/bp-tutorial.pdf)\n",
    "\n",
    "- Factor Graph is Global probabilty = product of local opinions\n",
    "- Factors or Potentials\n",
    "    - Factors are Tensors, 1 Dimension for every \n",
    "\n",
    "￼- Markov Random Field\n",
    "    - Undirected Graphical Model\n",
    "    - Joint distribution over tags Xi and words Wi\n",
    "    - The individual factors aren’t necessarily probabilities.\n",
    "\n",
    "<img  src=\"Screen Shot 2016-01-04 at 6.46.09 PM.png\"/>\n",
    "- Constrain each row of a factor to sum to one. Now Z = 1.\n",
    "- **If you normalize the matrices for each factor and the numbers are probablities you get a Bayes Net which in this case is a Hidden Markov Model Directed Graphical Model**\n",
    "<img  src=\"Screen Shot 2016-01-04 at 6.55.52 PM.png\"/>\n",
    "<img  src=\"Screen Shot 2016-01-04 at 6.56.01 PM.png\"/>\n",
    "- The graphs and learning for these models can be all different but inference like Belief Propagation works the same way\n",
    "- Given a factor Graph two common tasks both **NP-HARD** (so approximations)\n",
    "    - Compute the most likely joint assignment $$x^* = argmax_xp(X=x)$$\n",
    "    - Compute the marginal distribution of variable X_i for each value x_i $$p(X_i= x_i)$$\n",
    "    \n",
    "    <img  src=\"Screen Shot 2016-01-04 at 7.19.24 PM.png\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Logic Handout](http://www.cs.ucsb.edu/~benh/162_W16/handouts/handout1-FOL.pdf)\n",
    "- [Hilbert System for Logic](https://en.wikipedia.org/wiki/Hilbert_system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pedro Research:\n",
    "- [On the Optimality of the Simple Bayesian Classifier under Zero-One Loss](http://link.springer.com/article/10.1023/A:1007413511361#page-2)\n",
    "- [Trust Management for the Semantic Web](http://ml.cs.washington.edu/www/media/papers/richardson-al03a.pdf)\n",
    "- [Sum-Product Networks: A New Deep Architecture](http://homes.cs.washington.edu/~pedrod/papers/uai11a.pdf)\n",
    "- [Learning to Match Ontologies on the Semantic Web](http://homes.cs.washington.edu/~pedrod/papers/vldbj04.pdf)\n",
    "- [Large Scale Learning and Inference What we have learned with markov logic networks VIDEO](http://videolectures.net/nipsworkshops09_domingos_lsliwwlmln/)\n",
    "#### **[Class Probabilistic Graphical Models Advanced Methods](https://sites.google.com/site/cs228tspring2012/)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Snippet \n",
    "###### Derivaiton of Expected Loss\n",
    "---\n",
    "- Shows that minimize the squared loss function gets you the average or the Expected value\n",
    "--x--x---x-x-x-x-x--μ-----x-xxx-xx\n",
    "\\begin{equation}\n",
    "\\bar x = \\sum_1^n\\frac{x_i}{n} = average \\\\\n",
    "\\text{Minimize Function (convex): } L(\\mu) = \\sum_1^n (x_i-\\mu)^2 \\\\\n",
    "\\text{Take Derivative w.r.t $\\mu$ and set to 0: } \\frac{dL}{\\mu} = -2\\sum_1^n(x_i-\\mu) \\stackrel{?}{=} 0 \\\\\n",
    "\\sum_1^n(x_i) = n*\\mu \\\\ \n",
    "\\mu = \\frac{\\sum_1^n(x_i)}{n} = average! = \\bar x\n",
    "\\end{equation}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Chain\n",
    "- A FSM with probabilities on transitions\n",
    "    - $f_i = P(ever-returns-to-i | X_0 = i)$\n",
    "    - Transient if $f_i < 1$ then returns in X number of steps with a geometric probabiliy $f_i^{n-1}(1-f_i)$ n is number of steps\n",
    "        - if transient that means there is a dead state from its perspective\n",
    "    - Recurrent if $f_i = 1$ then returns $\\inf$ number of times to initial state\n",
    "        - no dead states starting from there\n",
    "    - Communitanitve (ignore transient states) if you can get from i -> j then you can get from j -> i\n",
    "        - i <-> j communicative states\n",
    "    - Split state space into communicative states\n",
    "        - if only one class => irreducible Markov Chain\n",
    "        - 2 or more means => Reducible Markov Chain(Each is a Markov Chain on its own)\n",
    "    - Reducible Markov Chain has more than 1 stationary distribution\n",
    "        - can be shown to be a block diagonal matrix\n",
    "    - Ignore transient assume, markov chain is irreducible, take state i and find the number of steps to return back to i. \n",
    "        - Number of steps $\\phi_1 \\phi_2 ...$\n",
    "        - d = Greatest Common Divisor($\\phi_1 \\phi_2 ...$)\n",
    "            - d is same for all i, period of Markov Chain\n",
    "        - Can split states in d subclasses $D_1, D_2, D_3... D_d$ if you start from $D_L$\n",
    "        - If d = 1 then aperiodic, otherwise periodic with period d\n",
    "    - Irreducible and aperiodic means $x(n) -> p$ as $h-> \\infty$ and p is unique, and $p_i > 0 $ for all i such a chain is called **ergodic**\n",
    "        - $p_i$ is long term proportion of steps spent in state i\n",
    "    - Rate of convergance [insert image](image)\n",
    "    - M 2-3, T 1-4, SH 5501\n",
    "\n",
    "\n",
    "\n",
    "-a\n",
    "- Include something on Shingling \n",
    "##### [Shingling](http://nlp.stanford.edu/IR-book/html/htmledition/near-duplicates-and-shingling-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Moment Generating Function and Cumulative Distrivution Function\n",
    "\n",
    "Discrete\n",
    "f(y) is the PMF of Y\n",
    "Conitnuos\n",
    "f(y) is pdf of Y\n",
    "\n",
    "Mean is first moment\n",
    "variacne is second moment\n",
    "second central moment is variance\n",
    "moment**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
